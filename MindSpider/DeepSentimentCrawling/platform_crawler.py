#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSentimentCrawlingæ¨¡å— - å¹³å°çˆ¬è™«ç®¡ç†å™¨
è´Ÿè´£é…ç½®å’Œè°ƒç”¨MediaCrawlerè¿›è¡Œå¤šå¹³å°çˆ¬å–
"""

import os
import re
import sys
import subprocess
import tempfile
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
import json
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

try:
    import config
except ImportError:
    raise ImportError("æ— æ³•å¯¼å…¥config.pyé…ç½®æ–‡ä»¶")

from platforms_config import SUPPORTED_PLATFORMS
from logging_utils import setup_platform_logger


class PlatformCrawler:
    """å¹³å°çˆ¬è™«ç®¡ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–å¹³å°çˆ¬è™«ç®¡ç†å™¨"""
        self.mediacrawler_path = Path(__file__).parent / "MediaCrawler"
        self.supported_platforms = SUPPORTED_PLATFORMS
        self.crawl_stats = {}

        # ç¡®ä¿MediaCrawlerç›®å½•å­˜åœ¨
        if not self.mediacrawler_path.exists():
            raise FileNotFoundError(f"MediaCrawlerç›®å½•ä¸å­˜åœ¨: {self.mediacrawler_path}")

        logger.info(f"åˆå§‹åŒ–å¹³å°çˆ¬è™«ç®¡ç†å™¨ï¼ŒMediaCrawlerè·¯å¾„: {self.mediacrawler_path}")

    def configure_mediacrawler_db(self):
        """
        é…ç½®MediaCrawlerä½¿ç”¨MindSpiderçš„æ•°æ®åº“

        æ³¨æ„ï¼šMediaCrawlerçš„db_config.pyå·²ç»é…ç½®ä¸ºè‡ªåŠ¨ä».envè¯»å–æ•°æ®åº“é…ç½®ï¼Œ
        å› æ­¤è¿™ä¸ªå‡½æ•°ç°åœ¨åªéœ€è¦éªŒè¯é…ç½®æ˜¯å¦æ­£ç¡®ã€‚
        """
        try:
            db_dialect = (config.settings.DB_DIALECT or "mysql").lower()
            db_type = (
                "PostgreSQL" if db_dialect in ("postgresql", "postgres") else "MySQL"
            )
            logger.info(f"å·²é…ç½®MediaCrawlerä½¿ç”¨MindSpider {db_type}æ•°æ®åº“")
            return True

        except Exception as e:
            logger.exception(f"é…ç½®MediaCrawleræ•°æ®åº“å¤±è´¥: {e}")
            return False

    def create_base_config(
        self,
        platform: str,
        keywords: List[str],
        crawler_type: str = "search",
        max_notes: int = 50,
    ) -> bool:
        """
        åˆ›å»ºMediaCrawlerçš„åŸºç¡€é…ç½®

        Args:
            platform: å¹³å°åç§°
            keywords: å…³é”®è¯åˆ—è¡¨
            crawler_type: çˆ¬å–ç±»å‹
            max_notes: æœ€å¤§çˆ¬å–æ•°é‡

        Returns:
            æ˜¯å¦é…ç½®æˆåŠŸ
        """
        try:
            # åˆ¤æ–­æ•°æ®åº“ç±»å‹ï¼Œç¡®å®š SAVE_DATA_OPTION
            db_dialect = (config.settings.DB_DIALECT or "mysql").lower()
            is_postgresql = db_dialect in ("postgresql", "postgres")
            save_data_option = "postgresql" if is_postgresql else "db"

            base_config_path = self.mediacrawler_path / "config" / "base_config.py"

            # å°†å…³é”®è¯åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
            sanitized_keywords = [
                kw.replace('"', "").replace("'", "").strip() for kw in keywords
            ]
            keywords_str = ",".join(sanitized_keywords)

            # è¯»å–åŸå§‹é…ç½®æ–‡ä»¶
            with open(base_config_path, "r", encoding="utf-8") as f:
                content = f.read()

            # æŠŠæ—§çš„å¤šè¡Œ CRAWLER_TYPE å—æ¸…ç†æ‰ï¼Œé¿å…æ®‹ç•™ç¼©è¿›
            content = re.sub(
                r"^CRAWLER_TYPE\s*=\s*\(\s*\r?\n.*?^\)\s*$",
                "",
                content,
                flags=re.MULTILINE | re.DOTALL,
            )

            def replace_or_append(pattern: str, replacement: str) -> None:
                nonlocal content
                if re.search(pattern, content, flags=re.MULTILINE):
                    content = re.sub(pattern, replacement, content, flags=re.MULTILINE)
                else:
                    if not content.endswith("\n"):
                        content += "\n"
                    content += replacement

            replace_or_append(
                r"^PLATFORM\s*=.*$",
                f'PLATFORM = "{platform}"  # å¹³å°ï¼Œxhs | dy | ks | bili | wb | tieba | zhihu',
            )
            replace_or_append(
                r"^KEYWORDS\s*=.*$",
                f'KEYWORDS = "{keywords_str}"  # å…³é”®è¯æœç´¢é…ç½®ï¼Œä»¥è‹±æ–‡é€—å·åˆ†éš”',
            )
            replace_or_append(
                r"^CRAWLER_TYPE\s*=.*$",
                f'CRAWLER_TYPE = "{crawler_type}"  # çˆ¬å–ç±»å‹ï¼Œsearch(å…³é”®è¯æœç´¢) | detail(å¸–å­è¯¦æƒ…)| creator(åˆ›ä½œè€…ä¸»é¡µæ•°æ®)',
            )
            replace_or_append(
                r"^SAVE_DATA_OPTION\s*=.*$",
                f'SAVE_DATA_OPTION = "{save_data_option}"  # csv or db or json or sqlite or postgresql',
            )
            replace_or_append(
                r"^CRAWLER_MAX_NOTES_COUNT\s*=.*$",
                f"CRAWLER_MAX_NOTES_COUNT = {max_notes}",
            )
            replace_or_append(
                r"^ENABLE_GET_COMMENTS\s*=.*$",
                "ENABLE_GET_COMMENTS = True",
            )
            replace_or_append(
                r"^CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES\s*=.*$",
                "CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES = 20",
            )

            # HEADLESS æ”¯æŒ .env å¼€å…³ï¼Œé»˜è®¤ä¸ºé…ç½®æ–‡ä»¶åŸå€¼
            headless_match = re.search(
                r"^HEADLESS\s*=\s*(True|False)", content, flags=re.MULTILINE
            )
            existing_headless_value = (
                headless_match.group(1) if headless_match else "True"
            )
            headless_env = os.getenv("MEDIACRAWLER_HEADLESS")
            if headless_env is not None:
                env_lower = headless_env.strip().lower()
                new_headless_value = (
                    "True" if env_lower in ("1", "true", "yes", "on") else "False"
                )
            else:
                new_headless_value = existing_headless_value

            replace_or_append(
                r"^HEADLESS\s*=.*$",
                f"HEADLESS = {new_headless_value}  # è¿è¡Œæ¨¡å¼ç”±é…ç½®æˆ– MEDIACRAWLER_HEADLESS æ§åˆ¶",
            )

            # å†™å…¥æ–°é…ç½®
            with open(base_config_path, "w", encoding="utf-8") as f:
                f.write(content)

            logger.info(
                f"å·²é…ç½® {platform} å¹³å°ï¼Œçˆ¬å–ç±»å‹: {crawler_type}ï¼Œå…³é”®è¯æ•°é‡: {len(keywords)}ï¼Œæœ€å¤§çˆ¬å–æ•°é‡: {max_notes}ï¼Œä¿å­˜æ•°æ®æ–¹å¼: {save_data_option}"
            )
            return True

        except Exception as e:
            logger.exception(f"åˆ›å»ºåŸºç¡€é…ç½®å¤±è´¥: {e}")
            return False

    def run_crawler(
        self,
        platform: str,
        keywords: List[str],
        login_type: str = "qrcode",
        max_notes: int = 50,
    ) -> Dict:
        """
        è¿è¡Œçˆ¬è™«

        Args:
            platform: å¹³å°åç§°
            keywords: å…³é”®è¯åˆ—è¡¨
            login_type: ç™»å½•æ–¹å¼
            max_notes: æœ€å¤§çˆ¬å–æ•°é‡

        Returns:
            çˆ¬å–ç»“æœç»Ÿè®¡
        """
        if platform not in self.supported_platforms:
            raise ValueError(f"ä¸æ”¯æŒçš„å¹³å°: {platform}")

        if not keywords:
            raise ValueError("å…³é”®è¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

        handler_id = setup_platform_logger(platform)

        start_message = f"\nå¼€å§‹çˆ¬å–å¹³å°: {platform}"
        start_message += f"\nå…³é”®è¯: {keywords[:5]}{'...' if len(keywords) > 5 else ''} (å…±{len(keywords)}ä¸ª)"
        logger.info(start_message)

        start_time = datetime.now()

        try:
            # é…ç½®æ•°æ®åº“
            if not self.configure_mediacrawler_db():
                return {"success": False, "error": "æ•°æ®åº“é…ç½®å¤±è´¥"}

            # åˆ›å»ºåŸºç¡€é…ç½®
            if not self.create_base_config(platform, keywords, "search", max_notes):
                return {"success": False, "error": "åŸºç¡€é…ç½®åˆ›å»ºå¤±è´¥"}

            # åˆ¤æ–­æ•°æ®åº“ç±»å‹ï¼Œç¡®å®š save_data_option
            db_dialect = (config.settings.DB_DIALECT or "mysql").lower()
            is_postgresql = db_dialect in ("postgresql", "postgres")
            save_data_option = "postgresql" if is_postgresql else "db"

            # æ„å»ºå‘½ä»¤
            cmd = [
                sys.executable,
                "main.py",
                "--platform",
                platform,
                "--lt",
                login_type,
                "--type",
                "search",
                "--save_data_option",
                save_data_option,
            ]

            logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")

            # ä½¿ç”¨ Popen å®æ—¶æ•è·è¾“å‡º
            # æ³¨æ„ï¼šä¸æŒ‡å®š text=True å’Œ encodingï¼Œä»¥äºŒè¿›åˆ¶æ¨¡å¼è¯»å–
            process = subprocess.Popen(
                cmd,
                cwd=self.mediacrawler_path,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                bufsize=1,  # è¡Œç¼“å†²
            )

            # å®æ—¶è¯»å–å¹¶è®°å½•è¾“å‡ºï¼Œæ™ºèƒ½å¤„ç†ç¼–ç 
            for line in process.stdout:
                # å°è¯•å¤šç§ç¼–ç è§£ç 
                decoded_line = None
                for encoding in ["utf-8", "gbk", "gb2312"]:
                    try:
                        decoded_line = line.decode(encoding).rstrip()
                        break
                    except (UnicodeDecodeError, AttributeError):
                        continue

                # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨ utf-8 å¹¶æ›¿æ¢é”™è¯¯å­—ç¬¦
                if decoded_line is None:
                    decoded_line = line.decode("utf-8", errors="replace").rstrip()

                if decoded_line:  # è·³è¿‡ç©ºè¡Œ
                    logger.info(f"[MediaCrawler] {decoded_line}")

            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            return_code = process.wait(timeout=3600)

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            # åˆ›å»ºç»Ÿè®¡ä¿¡æ¯
            crawl_stats = {
                "platform": platform,
                "keywords_count": len(keywords),
                "duration_seconds": duration,
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "return_code": return_code,
                "success": return_code == 0,
                "notes_count": 0,
                "comments_count": 0,
                "errors_count": 0,
            }

            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            self.crawl_stats[platform] = crawl_stats

            if return_code == 0:
                logger.info(f"âœ… {platform} çˆ¬å–å®Œæˆï¼Œè€—æ—¶: {duration:.1f}ç§’")
            else:
                logger.error(f"âŒ {platform} çˆ¬å–å¤±è´¥ï¼Œè¿”å›ç : {return_code}")

            return crawl_stats

        except subprocess.TimeoutExpired:
            logger.exception(f"âŒ {platform} çˆ¬å–è¶…æ—¶")
            return {"success": False, "error": "çˆ¬å–è¶…æ—¶", "platform": platform}
        except Exception as e:
            logger.exception(f"âŒ {platform} çˆ¬å–å¼‚å¸¸: {e}")
            return {"success": False, "error": str(e), "platform": platform}
        finally:
            logger.remove(handler_id)

    def _parse_crawl_output(
        self, output_lines: List[str], error_lines: List[str]
    ) -> Dict:
        """è§£æçˆ¬å–è¾“å‡ºï¼Œæå–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "notes_count": 0,
            "comments_count": 0,
            "errors_count": 0,
            "login_required": False,
        }

        # è§£æè¾“å‡ºè¡Œ
        for line in output_lines:
            if "æ¡ç¬”è®°" in line or "æ¡å†…å®¹" in line:
                try:
                    # æå–æ•°å­—
                    import re

                    numbers = re.findall(r"\d+", line)
                    if numbers:
                        stats["notes_count"] = int(numbers[0])
                except:
                    pass
            elif "æ¡è¯„è®º" in line:
                try:
                    import re

                    numbers = re.findall(r"\d+", line)
                    if numbers:
                        stats["comments_count"] = int(numbers[0])
                except:
                    pass
            elif "ç™»å½•" in line or "æ‰«ç " in line:
                stats["login_required"] = True

        # è§£æé”™è¯¯è¡Œ
        for line in error_lines:
            if "error" in line.lower() or "å¼‚å¸¸" in line:
                stats["errors_count"] += 1

        return stats

    def run_multi_platform_crawl_by_keywords(
        self,
        keywords: List[str],
        platforms: List[str],
        login_type: str = "qrcode",
        max_notes_per_keyword: int = 50,
    ) -> Dict:
        """
        åŸºäºå…³é”®è¯çš„å¤šå¹³å°çˆ¬å– - æ¯ä¸ªå…³é”®è¯åœ¨æ‰€æœ‰å¹³å°ä¸Šéƒ½è¿›è¡Œçˆ¬å–

        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            platforms: å¹³å°åˆ—è¡¨
            login_type: ç™»å½•æ–¹å¼
            max_notes_per_keyword: æ¯ä¸ªå…³é”®è¯åœ¨æ¯ä¸ªå¹³å°çš„æœ€å¤§çˆ¬å–æ•°é‡

        Returns:
            æ€»ä½“çˆ¬å–ç»Ÿè®¡
        """

        start_message = f"\nğŸš€ å¼€å§‹å…¨å¹³å°å…³é”®è¯çˆ¬å–"
        start_message += f"\n   å…³é”®è¯æ•°é‡: {len(keywords)}"
        start_message += f"\n   å¹³å°æ•°é‡: {len(platforms)}"
        start_message += f"\n   ç™»å½•æ–¹å¼: {login_type}"
        start_message += (
            f"\n   æ¯ä¸ªå…³é”®è¯åœ¨æ¯ä¸ªå¹³å°çš„æœ€å¤§çˆ¬å–æ•°é‡: {max_notes_per_keyword}"
        )
        start_message += f"\n   æ€»çˆ¬å–ä»»åŠ¡: {len(keywords)} Ã— {len(platforms)} = {len(keywords) * len(platforms)}"
        logger.info(start_message)

        total_stats = {
            "total_keywords": len(keywords),
            "total_platforms": len(platforms),
            "total_tasks": len(keywords) * len(platforms),
            "successful_tasks": 0,
            "failed_tasks": 0,
            "total_notes": 0,
            "total_comments": 0,
            "keyword_results": {},
            "platform_summary": {},
        }

        # åˆå§‹åŒ–å¹³å°ç»Ÿè®¡
        for platform in platforms:
            total_stats["platform_summary"][platform] = {
                "successful_keywords": 0,
                "failed_keywords": 0,
                "total_notes": 0,
                "total_comments": 0,
            }

        # å¯¹æ¯ä¸ªå¹³å°ä¸€æ¬¡æ€§çˆ¬å–æ‰€æœ‰å…³é”®è¯
        for platform in platforms:
            logger.info(f"\nğŸ“ åœ¨ {platform} å¹³å°çˆ¬å–æ‰€æœ‰å…³é”®è¯")
            logger.info(
                f"   å…³é”®è¯: {', '.join(keywords[:5])}{'...' if len(keywords) > 5 else ''}"
            )

            try:
                # ä¸€æ¬¡æ€§ä¼ é€’æ‰€æœ‰å…³é”®è¯ç»™å¹³å°
                result = self.run_crawler(
                    platform, keywords, login_type, max_notes_per_keyword
                )

                if result.get("success"):
                    total_stats["successful_tasks"] += len(keywords)
                    total_stats["platform_summary"][platform]["successful_keywords"] = (
                        len(keywords)
                    )

                    notes_count = result.get("notes_count", 0)
                    comments_count = result.get("comments_count", 0)

                    total_stats["total_notes"] += notes_count
                    total_stats["total_comments"] += comments_count
                    total_stats["platform_summary"][platform][
                        "total_notes"
                    ] = notes_count
                    total_stats["platform_summary"][platform][
                        "total_comments"
                    ] = comments_count

                    # ä¸ºæ¯ä¸ªå…³é”®è¯è®°å½•ç»“æœ
                    for keyword in keywords:
                        if keyword not in total_stats["keyword_results"]:
                            total_stats["keyword_results"][keyword] = {}
                        total_stats["keyword_results"][keyword][platform] = result

                    logger.info(
                        f"   âœ… æˆåŠŸ: {notes_count} æ¡å†…å®¹, {comments_count} æ¡è¯„è®º"
                    )
                else:
                    total_stats["failed_tasks"] += len(keywords)
                    total_stats["platform_summary"][platform]["failed_keywords"] = len(
                        keywords
                    )

                    # ä¸ºæ¯ä¸ªå…³é”®è¯è®°å½•å¤±è´¥ç»“æœ
                    for keyword in keywords:
                        if keyword not in total_stats["keyword_results"]:
                            total_stats["keyword_results"][keyword] = {}
                        total_stats["keyword_results"][keyword][platform] = result

                    logger.error(f"   âŒ å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

            except Exception as e:
                total_stats["failed_tasks"] += len(keywords)
                total_stats["platform_summary"][platform]["failed_keywords"] = len(
                    keywords
                )
                error_result = {"success": False, "error": str(e)}

                # ä¸ºæ¯ä¸ªå…³é”®è¯è®°å½•å¼‚å¸¸ç»“æœ
                for keyword in keywords:
                    if keyword not in total_stats["keyword_results"]:
                        total_stats["keyword_results"][keyword] = {}
                    total_stats["keyword_results"][keyword][platform] = error_result

                logger.error(f"   âŒ å¼‚å¸¸: {e}")

        # æ‰“å°è¯¦ç»†ç»Ÿè®¡
        finish_message = f"\nğŸ“Š å…¨å¹³å°å…³é”®è¯çˆ¬å–å®Œæˆ!"
        finish_message += f"\n   æ€»ä»»åŠ¡: {total_stats['total_tasks']}"
        finish_message += f"\n   æˆåŠŸ: {total_stats['successful_tasks']}"
        finish_message += f"\n   å¤±è´¥: {total_stats['failed_tasks']}"
        finish_message += f"\n   æˆåŠŸç‡: {total_stats['successful_tasks']/total_stats['total_tasks']*100:.1f}%"
        finish_message += f"\n   æ€»å†…å®¹: {total_stats['total_notes']} æ¡"
        finish_message += f"\n   æ€»è¯„è®º: {total_stats['total_comments']} æ¡"
        logger.info(finish_message)

        platform_summary_message = f"\nï¿½ å„å¹³å°ç»Ÿè®¡:"
        for platform, stats in total_stats["platform_summary"].items():
            success_rate = (
                stats["successful_keywords"] / len(keywords) * 100 if keywords else 0
            )
            platform_summary_message += f"\n   {platform}: {stats['successful_keywords']}/{len(keywords)} å…³é”®è¯æˆåŠŸ ({success_rate:.1f}%), "
            platform_summary_message += f"{stats['total_notes']} æ¡å†…å®¹"
        logger.info(platform_summary_message)

        return total_stats

    def get_crawl_statistics(self) -> Dict:
        """è·å–çˆ¬å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "platforms_crawled": list(self.crawl_stats.keys()),
            "total_platforms": len(self.crawl_stats),
            "detailed_stats": self.crawl_stats,
        }

    def save_crawl_log(self, log_path: str = None):
        """ä¿å­˜çˆ¬å–æ—¥å¿—"""
        if not log_path:
            log_path = f"crawl_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        try:
            with open(log_path, "w", encoding="utf-8") as f:
                json.dump(self.crawl_stats, f, ensure_ascii=False, indent=2)
            logger.info(f"çˆ¬å–æ—¥å¿—å·²ä¿å­˜åˆ°: {log_path}")
        except Exception as e:
            logger.exception(f"ä¿å­˜çˆ¬å–æ—¥å¿—å¤±è´¥: {e}")


if __name__ == "__main__":
    # æµ‹è¯•å¹³å°çˆ¬è™«ç®¡ç†å™¨
    crawler = PlatformCrawler()

    # æµ‹è¯•é…ç½®
    test_keywords = ["ç§‘æŠ€", "AI", "ç¼–ç¨‹"]
    result = crawler.run_crawler("xhs", test_keywords, max_notes=5)

    logger.info(f"æµ‹è¯•ç»“æœ: {result}")
    logger.info("å¹³å°çˆ¬è™«ç®¡ç†å™¨æµ‹è¯•å®Œæˆï¼")
